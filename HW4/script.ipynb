{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in to training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pd.read_csv('Dataset/train_X.csv')\n",
    "y_all = pd.read_csv('Dataset/train_y.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the one-hot encoder, which converts the string of sequence of amino acids to a set of values, either one or zero. Each value represents the existance of a certain amino acid on a specific position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    X = np.array(X_all['ConstructedAASeq_cln'].apply(lambda x: list(x)).to_list())\n",
    "    enc.fit(X)\n",
    "    return enc, enc.transform(df['ConstructedAASeq_cln'].apply(lambda x: list(x)).to_list()).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load all the CSV files in the descriptors. These files describe some properties of each amino acid, and all data are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load descriptors\n",
    "dpps = pd.read_csv('Dataset/descriptors/DPPS.csv', header=2).drop('AA_3', axis=1)\n",
    "df_combined = dpps\n",
    "for file_name in ['Physical', 'Physical', 'ST-scale', 'T-scale', 'VHSE-scale', 'Z-scale']:\n",
    "    df = pd.read_csv(f'Dataset/descriptors/{file_name}.csv', header=2).drop('AA_3', axis=1)\n",
    "    df_combined = df_combined.merge(df, on='AA_1', how='inner')\n",
    "df_combined = df_combined.set_index('AA_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After combining the tables, we can see there are 38 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>...</th>\n",
       "      <th>VHSE2</th>\n",
       "      <th>VHSE3</th>\n",
       "      <th>VHSE4</th>\n",
       "      <th>VHSE5</th>\n",
       "      <th>VHSE6</th>\n",
       "      <th>VHSE7</th>\n",
       "      <th>VHSE8</th>\n",
       "      <th>Z(1)</th>\n",
       "      <th>Z(2)</th>\n",
       "      <th>Z(3)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AA_1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>-1.02</td>\n",
       "      <td>-2.88</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-6.15</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>1.99</td>\n",
       "      <td>4.13</td>\n",
       "      <td>-4.41</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.04</td>\n",
       "      <td>-9.06</td>\n",
       "      <td>6.71</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>-2.19</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-5.71</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>3.22</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>-6.60</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-3.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>-7.36</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-2.68</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.21</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.42</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>3.11</td>\n",
       "      <td>-2.98</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>1.57</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        D1    D2    D3    D4    D5    D6    D7    D8    D9   D10  ...  VHSE2  \\\n",
       "AA_1                                                              ...          \n",
       "A    -1.02 -2.88 -0.56  0.36 -6.15 -1.68  0.04 -2.51 -1.94 -0.01  ...  -1.11   \n",
       "R     1.99  4.13 -4.41 -1.02  4.78  3.04 -9.06  6.71  4.41  0.07  ...   1.45   \n",
       "N    -2.19  1.86  0.38 -0.13 -2.30  1.41 -5.71 -1.11  1.73 -0.19  ...   0.00   \n",
       "D    -6.60  3.32  1.61  0.36 -3.25  1.95 -7.36  0.14  1.24 -0.15  ...   0.67   \n",
       "C     0.21  1.12  3.42 -0.68 -2.27 -1.22  3.11 -2.98 -1.70  1.57  ...  -1.67   \n",
       "\n",
       "      VHSE3  VHSE4  VHSE5  VHSE6  VHSE7  VHSE8  Z(1)  Z(2)  Z(3)  \n",
       "AA_1                                                              \n",
       "A     -1.35  -0.92   0.02  -0.91   0.36  -0.48  0.07 -1.73  0.09  \n",
       "R      1.24   1.27   1.55   1.47   1.30   0.83  2.88  2.52 -3.44  \n",
       "N     -0.37   0.69  -0.55   0.85   0.73  -0.80  3.22  1.45  0.84  \n",
       "D     -0.41  -0.01  -2.68   1.31   0.03   0.56  3.64  1.13  2.36  \n",
       "C     -0.46  -0.21   0.00   1.20  -1.61  -0.19  0.71 -0.97  4.13  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we encode the data, and train a baseline logistic regression model with this encoding. We can see the accuracy is 0.9006, which is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9006\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoder, one_hot_encoded = one_hot_encode(X_all)\n",
    "brightnesses = y_all['Brightness_Class']\n",
    "X_train_oh, X_test_oh, y_train, y_test = train_test_split(one_hot_encoded, brightnesses, test_size=0.2, random_state=5)\n",
    "\n",
    "# Baseline One-Hot Encoding Logistic Regression Model\n",
    "# Train the model (~30s)\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=5)\n",
    "logreg.fit(X_train_oh, y_train.values)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(X_test_oh)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following codes, we will create a dataframe to examine the coeficients of the logistic regression model, for each amino acid-posiiton combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>Position</th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.214272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K</td>\n",
       "      <td>1</td>\n",
       "      <td>0.331179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.137291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>H</td>\n",
       "      <td>235</td>\n",
       "      <td>-0.265211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>N</td>\n",
       "      <td>235</td>\n",
       "      <td>0.165153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>S</td>\n",
       "      <td>235</td>\n",
       "      <td>0.370086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>Y</td>\n",
       "      <td>235</td>\n",
       "      <td>0.031901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>K</td>\n",
       "      <td>236</td>\n",
       "      <td>0.034112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1930 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AA  Position      Coef\n",
       "0     S         0  0.034112\n",
       "1     E         1 -0.214272\n",
       "2     K         1  0.331179\n",
       "3     M         1  0.513529\n",
       "4     N         1 -0.137291\n",
       "...  ..       ...       ...\n",
       "1925  H       235 -0.265211\n",
       "1926  N       235  0.165153\n",
       "1927  S       235  0.370086\n",
       "1928  Y       235  0.031901\n",
       "1929  K       236  0.034112\n",
       "\n",
       "[1930 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant_nums = []\n",
    "for category in one_hot_encoder.categories_:\n",
    "    variant_nums.append(len(category))\n",
    "\n",
    "positions = []\n",
    "i = 0\n",
    "for n in variant_nums:\n",
    "    for _ in range(n):\n",
    "        positions.append(i)\n",
    "    i += 1\n",
    "\n",
    "one_hot_features = pd.DataFrame()\n",
    "one_hot_features['AA'] = [i for s in one_hot_encoder.categories_ for i in s]\n",
    "one_hot_features['Position'] = positions\n",
    "one_hot_features['Coef'] = logreg.coef_[0]\n",
    "\n",
    "one_hot_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to keep only important positions, namely the positions that include some amino acids that has a coeficient greater than 3 or less than -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_positions = one_hot_features[abs(one_hot_features['Coef']) > 3]['Position'].values\n",
    "unimportant_indexs = one_hot_features[one_hot_features['Position'].apply(lambda x: x not in important_positions)].index\n",
    "one_hot_unimportant = one_hot_encoded[:, unimportant_indexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the original dataset for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, brightnesses, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the descriptor files, we take all the features and only on important positions. For each of them, we add 38 features with the value of the given amino acid on that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_des_features(df, im_pos):\n",
    "    matrixs = []\n",
    "    for p in im_pos:\n",
    "        matrix = []\n",
    "        for aa in df['ConstructedAASeq_cln'].apply(lambda x: x[p]).to_list():\n",
    "            matrix.append(df_combined.loc[aa].values)\n",
    "        matrixs.append(np.array(matrix))\n",
    "\n",
    "    return np.hstack(matrixs)\n",
    "\n",
    "# ~8s\n",
    "X_train_des = create_des_features(X_train, important_positions)\n",
    "X_test_des = create_des_features(X_test, important_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data of all unimportant positions for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oh, X_test_oh, y_train, y_test = train_test_split(one_hot_unimportant, brightnesses, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the two parts we have now: the important positions which replaced by the 38 features in the descriptors, and the unimportant positions which still use the one-hot encoding construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and combine the one-hot on unimportant positions and descripters on important positions\n",
    "def combine_and_scale(oh, des):\n",
    "    scaled_numeric_features = np.hstack((oh, des))\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(scaled_numeric_features)\n",
    "\n",
    "X_train_oh_des = combine_and_scale(X_train_oh, X_train_des)\n",
    "X_test_oh_des = combine_and_scale(X_test_oh, X_test_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, which gives 0.8969 accuracy, which is slightly lower, but may be more general for unseen data (which has been proved to be true given the result on Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8969\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=5)\n",
    "logreg.fit(X_train_oh_des, y_train.values)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(X_test_oh_des)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we train the model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final model\n",
    "X_des = create_des_features(X_all, important_positions)\n",
    "X_oh_des = combine_and_scale(one_hot_encoded, X_des)\n",
    "\n",
    "final_model = LogisticRegression(max_iter=1000, random_state=5)\n",
    "final_model.fit(X_oh_des, brightnesses.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the prediction to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make File to Submit\n",
    "\n",
    "def write_ans(predictions):\n",
    "    out = pd.read_csv('Dataset/y_sample_submission.csv')\n",
    "    out['Brightness_Class'] = predictions\n",
    "    out.to_csv('Dataset/predictions.csv', index=False)\n",
    "    \n",
    "X_to_predict = pd.read_csv('Dataset/test_X.csv')\n",
    "predictions = final_model.predict(\n",
    "    combine_and_scale(one_hot_encode(X_to_predict)[1], create_des_features(X_to_predict, important_positions))\n",
    ")\n",
    "write_ans(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
